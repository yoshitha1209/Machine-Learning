{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1R0XI4Q9tHbrmXdci0iANmScOcSH49shM",
      "authorship_tag": "ABX9TyPjTExRhVVGw++q+jKRX7OO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoshitha1209/Machine-Learning/blob/main/23003_lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/Copy of training_mathbert(1).xlsx\")  # Update with your actual file path\n",
        "\n",
        "# Split into features and target variable\n",
        "X = df.drop(columns=['output'])  # Features (embeddings)\n",
        "y = df['output']  # Target variable (correctness score)\n",
        "\n",
        "# Define hyperparameter search space (FIXED max_features)\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 300),  # Number of trees\n",
        "    'max_depth': [None] + list(range(5, 50, 5)),  # Tree depth\n",
        "    'min_samples_split': randint(2, 10),  # Minimum samples required to split a node\n",
        "    'min_samples_leaf': randint(1, 10),  # Minimum samples per leaf node\n",
        "    'max_features': ['sqrt', 'log2', None]  # Removed 'auto'\n",
        "}\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Setup RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    rf, param_distributions=param_dist,\n",
        "    n_iter=20, cv=5, scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# A5 - Clustering#\n",
        "\n",
        "# Agglomerative Clustering (Hierarchical)\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "sch.dendrogram(sch.linkage(X, method='ward'), truncate_mode='level', p=10)  # Limits depth for clarity\n",
        "plt.title(\"Dendrogram for Hierarchical Clustering\")\n",
        "plt.xlabel(\"Data Points\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n",
        "\n",
        "# Fit Agglomerative Clustering with chosen n_clusters (adjusted based on dendrogram)\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=36, linkage='ward')\n",
        "agg_labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Add cluster labels to the dataset\n",
        "df['AggloCluster'] = agg_labels"
      ],
      "metadata": {
        "id": "A8sQwDtWyRea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PI8gnbIODnCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXrCNk1Ow1kX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "# DBSCAN model\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n",
        "\n",
        "# Fit and predict clusters\n",
        "dbscan_labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Add DBSCAN cluster labels to dataset\n",
        "df['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "# Count number of noise points (-1 means noise)\n",
        "n_noise = np.sum(dbscan_labels == -1)\n",
        "print(f\"Number of Noise Points (DBSCAN): {n_noise}\")\n",
        "\n",
        "#Evaluating Clustering\n",
        "\n",
        "# Silhouette Score for Agglomerative Clustering\n",
        "agg_silhouette = silhouette_score(X, agg_labels)\n",
        "print(f\"Silhouette Score (Agglomerative): {agg_silhouette}\")\n",
        "\n",
        "# Silhouette Score for DBSCAN (excluding noise points)\n",
        "dbscan_silhouette = silhouette_score(X[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
        "print(f\"Silhouette Score (DBSCAN, excluding noise): {dbscan_silhouette}\")\n",
        "\n",
        "\n",
        "# Davies-Bouldin Score for Agglomerative Clustering\n",
        "agg_db_score = davies_bouldin_score(X, agg_labels)\n",
        "print(f\"Davies-Bouldin Score (Agglomerative): {agg_db_score}\")\n",
        "\n",
        "# Davies-Bouldin Score for DBSCAN (excluding noise points)\n",
        "if len(set(dbscan_labels)) > 1:  # Avoids error if all points are noise\n",
        "    dbscan_db_score = davies_bouldin_score(X[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
        "    print(f\"Davies-Bouldin Score (DBSCAN): {dbscan_db_score}\")\n",
        "else:\n",
        "    print(\"DBSCAN formed only one cluster or all points are noise.\")\n",
        "\n",
        "\n",
        "# Calinski-Harabasz Score for Agglomerative Clustering\n",
        "agg_ch_score = calinski_harabasz_score(X, agg_labels)\n",
        "print(f\"Calinski-Harabasz Score (Agglomerative): {agg_ch_score}\")\n",
        "\n",
        "# Calinski-Harabasz Score for DBSCAN (excluding noise)\n",
        "if len(set(dbscan_labels)) > 1:\n",
        "    dbscan_ch_score = calinski_harabasz_score(X[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
        "    print(f\"Calinski-Harabasz Score (DBSCAN): {dbscan_ch_score}\")\n",
        "else:\n",
        "    print(\"DBSCAN formed only one cluster or all points are noise.\")"
      ]
    }
  ]
}