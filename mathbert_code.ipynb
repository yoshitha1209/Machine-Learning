{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11610218,"sourceType":"datasetVersion","datasetId":7282366},{"sourceId":11610990,"sourceType":"datasetVersion","datasetId":7282898},{"sourceId":32631,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":27319,"modelId":38680}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install necessary packages (skip if on Kaggle)\n# !pip install transformers openpyxl tqdm scikit-learn --quiet\n\n# Step 2: Load dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\n\n# Load dataset\ndataset_path = '/kaggle/input/trainingdataset/training new 1.xlsx'\ndf = pd.read_excel(dataset_path)\n\nprint(\"✅ Columns:\", df.columns)\n\ntexts = df['input'].astype(str).tolist()\nlabels = df['Class'].tolist()\n\n# Encode class labels\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(labels)\nnum_labels = len(set(encoded_labels))\n\n# Train-validation split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    texts, encoded_labels, test_size=0.1, stratify=encoded_labels, random_state=42\n)\n\n# Step 3: Tokenize and Dataset class\nfrom transformers import AutoTokenizer\n\n# Load tokenizer (using local path for mathbert model if valid)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/mathbert/other/model/1\")\n\nclass MathBERTDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_len)\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n            'labels': torch.tensor(self.labels[idx])\n        }\n\ntrain_dataset = MathBERTDataset(train_texts, train_labels, tokenizer)\nval_dataset = MathBERTDataset(val_texts, val_labels, tokenizer)\n\n# Step 4: Fine-Tune MathBERT for Classification\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\n# Load model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"/kaggle/input/mathbert/other/model/1\", num_labels=num_labels,\n    output_hidden_states=True  # We need hidden states for embeddings\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./mathbert_output\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=5,\n    save_steps=500,\n    save_total_limit=2,\n    logging_steps=100,\n    report_to=\"none\"\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model\ntrainer.train()\n\n# Step 5: Extract Fine-Tuned MathBERT Embeddings\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\nfinetuned_embeddings = []\n\nwith torch.no_grad():\n    for text in tqdm(texts, desc=\"Extracting Fine-Tuned MathBERT Embeddings\"):\n        # Tokenizing the input text\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n\n        # Forward pass through the model\n        output = model(**inputs)\n\n        # Extract the embeddings (using the last hidden state)\n        mean_emb = output.hidden_states[-1].mean(dim=1)  # Average across all tokens\n        finetuned_embeddings.append(mean_emb.squeeze(0).cpu())  # Remove batch dimension\n\n# Saving the fine-tuned embeddings in a DataFrame\nfinetuned_df = pd.DataFrame([emb.numpy() for emb in finetuned_embeddings])\nfinetuned_df['Class'] = labels\nfinetuned_df.to_excel('mathbert_fine_tuned_embeddings.xlsx', index=False)\n\nprint(\"✅ Fine-tuned MathBERT embeddings saved!\")\n\n# Step 6: Extract Pretrained MathBERT Embeddings\nfrom transformers import AutoModel\n\n# Load the pretrained model (using the local path for MathBERT if valid)\npretrained_model = AutoModel.from_pretrained(\n    \"/kaggle/input/mathbert/other/model/1\", output_hidden_states=True\n).to(device)\npretrained_model.eval()\n\npretrained_embeddings = []\n\nwith torch.no_grad():\n    for text in tqdm(texts, desc=\"Extracting Pretrained MathBERT Embeddings\"):\n        # Tokenizing the input text\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n\n        # Forward pass through the pretrained model\n        output = pretrained_model(**inputs)\n\n        # Extract the embeddings (using the last hidden state)\n        mean_emb = output.hidden_states[-1].mean(dim=1)  # Average across all tokens\n        pretrained_embeddings.append(mean_emb.squeeze(0).cpu())  # Remove batch dimension\n\n# Saving the pretrained embeddings in a DataFrame\npretrained_df = pd.DataFrame([emb.numpy() for emb in pretrained_embeddings])\npretrained_df['Class'] = labels\npretrained_df.to_excel('mathbert_pre_trained_embeddings.xlsx', index=False)\n\nprint(\"✅ Pretrained MathBERT embeddings saved!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T19:40:05.795248Z","iopub.execute_input":"2025-04-29T19:40:05.796001Z","iopub.status.idle":"2025-04-29T19:50:56.531482Z","shell.execute_reply.started":"2025-04-29T19:40:05.795975Z","shell.execute_reply":"2025-04-29T19:50:56.530826Z"}},"outputs":[{"name":"stdout","text":"✅ Columns: Index(['input', 'Class'], dtype='object')\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/mathbert/other/model/1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [945/945 07:56, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.056100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.002700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.849500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.789100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.712200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.697600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.588700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.534500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.529600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nExtracting Fine-Tuned MathBERT Embeddings: 100%|██████████| 1680/1680 [01:01<00:00, 27.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Fine-tuned MathBERT embeddings saved!\n","output_type":"stream"},{"name":"stderr","text":"Extracting Pretrained MathBERT Embeddings: 100%|██████████| 1680/1680 [01:02<00:00, 26.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Pretrained MathBERT embeddings saved!\n","output_type":"stream"}],"execution_count":2}]}